# GenAlign Configuration File

# Model Configuration
models:
  generator:
    name: "meta-llama/Llama-3.1-8B-Instruct"
    cache_dir: "./cache"
    quantization: "4bit"  # 4bit, 8bit, or none
    use_lora: true
    lora_rank: 16
    lora_alpha: 32
    lora_dropout: 0.1
  
  classifier:
    name: "roberta-base"
    cache_dir: "./cache"
    num_labels: 2  # Will be updated based on dataset

# Training Configuration
training:
  batch_size: 4
  learning_rate: 1e-5
  num_epochs: 3
  gradient_accumulation_steps: 4
  max_grad_norm: 1.0
  warmup_steps: 100
  weight_decay: 0.01
  mixed_precision: "fp16"  # fp16, bf16, or none

# Generation Configuration
generation:
  num_icl_examples: 4
  temperature: 0.7
  max_length: 512
  num_samples_per_iteration: 100
  top_p: 0.9
  do_sample: true

# Reward Configuration
reward:
  weights:
    inter_class_distance: 1.0
    intra_class_distance: 1.0
    golden_loss: 1.0
  normalization:
    use_min_max: true
    clip_rewards: true
    clip_value: 5.0

# Convergence Criteria
convergence:
  max_iterations: 50
  patience: 5
  min_delta: 0.01
  early_stopping: true

# Data Configuration
data:
  golden_data_path: "./data/yelp_polarity.csv"
  text_column: "text"
  label_column: "label"
  train_split: 0.8
  val_split: 0.2
  max_length: 512

# Logging Configuration
logging:
  use_wandb: true
  project_name: "genalign-rl"
  log_interval: 10
  save_interval: 5
  output_dir: "./outputs"

# Hardware Configuration
hardware:
  device: "auto"  # auto, cuda, cpu
  num_workers: 4
  pin_memory: true
