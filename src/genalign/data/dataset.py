import pandas as pd
import torch
from torch.utils.data import Dataset, DataLoader
from typing import List, Dict, Any, Optional, Tuple
import os
from sklearn.model_selection import train_test_split


class GoldenDataset(Dataset):
    """Dataset class for loading and managing golden (real) data."""
    
    def __init__(
        self,
        data_path: str,
        text_column: str = "text",
        label_column: str = "label",
        max_length: int = 512,
        train_split: float = 0.8,
        val_split: float = 0.2,
        random_state: int = 42
    ):
        """
        Initialize the golden dataset.
        
        Args:
            data_path: Path to the CSV file containing golden data
            text_column: Name of the text column
            label_column: Name of the label column
            max_length: Maximum sequence length
            train_split: Fraction of data to use for training
            val_split: Fraction of data to use for validation
            random_state: Random seed for reproducibility
        """
        self.data_path = data_path
        self.text_column = text_column
        self.label_column = label_column
        self.max_length = max_length
        
        # Load and process data
        self.df = pd.read_csv(data_path)
        self.texts = self.df[text_column].tolist()
        self.labels = self.df[label_column].tolist()
        
        # Get unique labels and create label mapping
        self.unique_labels = sorted(list(set(self.labels)))
        self.label_to_id = {label: idx for idx, label in enumerate(self.unique_labels)}
        self.id_to_label = {idx: label for label, idx in self.label_to_id.items()}
        self.num_labels = len(self.unique_labels)
        
        # Convert labels to IDs
        self.label_ids = [self.label_to_id[label] for label in self.labels]
        
        # Split data
        self.train_texts, self.val_texts, self.train_labels, self.val_labels = train_test_split(
            self.texts, self.label_ids, 
            test_size=val_split, 
            random_state=random_state,
            stratify=self.label_ids
        )
        
        # Further split train into train and test if needed
        if train_split < 1.0:
            remaining_size = 1.0 - val_split
            actual_train_size = train_split / remaining_size
            if actual_train_size < 1.0:  # Only split if we need to
                self.train_texts, _, self.train_labels, _ = train_test_split(
                    self.train_texts, self.train_labels,
                    train_size=actual_train_size,
                    random_state=random_state,
                    stratify=self.train_labels
                )
    
    def get_train_data(self) -> Tuple[List[str], List[int]]:
        """Get training data."""
        return self.train_texts, self.train_labels
    
    def get_val_data(self) -> Tuple[List[str], List[int]]:
        """Get validation data."""
        return self.val_texts, self.val_labels
    
    def get_all_data(self) -> Tuple[List[str], List[int]]:
        """Get all data."""
        return self.texts, self.label_ids
    
    def get_class_examples(self, class_id: int, num_examples: int = None) -> List[Tuple[str, int]]:
        """Get examples from a specific class."""
        examples = [(text, label_id) for text, label_id in zip(self.texts, self.label_ids) 
                   if label_id == class_id]
        if num_examples is not None:
            examples = examples[:num_examples]
        return examples
    
    def get_label_distribution(self) -> Dict[int, int]:
        """Get the distribution of labels."""
        distribution = {}
        for label_id in self.label_ids:
            distribution[label_id] = distribution.get(label_id, 0) + 1
        return distribution
    
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        return {
            "text": self.texts[idx],
            "label": self.label_ids[idx],
            "label_name": self.id_to_label[self.label_ids[idx]]
        }


class SyntheticDataset(Dataset):
    """Dataset class for managing synthetic data generated by the model."""
    
    def __init__(self, max_length: int = 512):
        """
        Initialize the synthetic dataset.
        
        Args:
            max_length: Maximum sequence length
        """
        self.max_length = max_length
        self.texts = []
        self.labels = []
        self.label_names = []
        self.iteration = 0
    
    def add_samples(self, texts: List[str], labels: List[int], label_names: List[str], iteration: int):
        """
        Add new synthetic samples to the dataset.
        
        Args:
            texts: List of generated texts
            labels: List of corresponding label IDs
            label_names: List of corresponding label names
            iteration: Current iteration number
        """
        self.texts.extend(texts)
        self.labels.extend(labels)
        self.label_names.extend(label_names)
        self.iteration = iteration
    
    def clear(self):
        """Clear all synthetic data."""
        self.texts = []
        self.labels = []
        self.label_names = []
        self.iteration = 0
    
    def get_data(self) -> Tuple[List[str], List[int]]:
        """Get all synthetic data."""
        return self.texts, self.labels
    
    def get_label_distribution(self) -> Dict[int, int]:
        """Get the distribution of labels in synthetic data."""
        distribution = {}
        for label in self.labels:
            distribution[label] = distribution.get(label, 0) + 1
        return distribution
    
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        return {
            "text": self.texts[idx],
            "label": self.labels[idx],
            "label_name": self.label_names[idx],
            "iteration": self.iteration
        }


def create_dataloader(
    dataset: Dataset,
    batch_size: int = 32,
    shuffle: bool = True,
    num_workers: int = 4,
    pin_memory: bool = True
) -> DataLoader:
    """
    Create a DataLoader for the given dataset.
    
    Args:
        dataset: The dataset to create a DataLoader for
        batch_size: Batch size
        shuffle: Whether to shuffle the data
        num_workers: Number of worker processes
        pin_memory: Whether to pin memory for faster GPU transfer
    
    Returns:
        DataLoader instance
    """
    return DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=shuffle,
        num_workers=num_workers,
        pin_memory=pin_memory,
        collate_fn=lambda x: x  # Return raw data for custom processing
    )
